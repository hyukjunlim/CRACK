\documentclass{article}

% \PassOptionsToPackage{numbers, compress}{natbib}
\usepackage[preprint]{neurips_2024}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{natbib}
\usepackage{amsmath}
\bibliographystyle{plainnat}

\title{MPFlow: A Flow-based Surrogate Model for Message Passing in Graph Neural Networks}

\author{
  Hyukjun Lim, Seokhyun Choung, Jeong Woo Han\thanks{Corresponding Author.} \\
  Department of Materials Science and Engineering \\
  Seoul National University \\
  \texttt{\{hyukjunlim, schoung9967, jwhan98\}@snu.ac.kr} \\
  % \And
  % Hyukjun Lim \\
  % Department of Materials Science and Engineering \\
  % Seoul National University \\
  % \texttt{hyukjunlim@snu.ac.kr} \\
  % \And
  % Hyukjun Lim \\
  % Department of Materials Science and Engineering \\
  % Seoul National University \\
  % \texttt{hyukjunlim@snu.ac.kr} \\
}


\begin{document}


\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Graph Neural Networks (GNNs) have demonstrated remarkable success across various domains, but their iterative message passing mechanism often incurs significant computational overhead, especially for deeper architectures. 
We present MPFlow, a flow-based surrogate model that predicts the outcome of message passing operations without explicitly computing intermediate states. 
Our approach first obtains global graph representations through either mean pooling or spherical harmonic pooling, depending on the GNN's invariance properties, and then learns a continuous velocity field that governs their evolution. 
By formulating message passing as a flow matching problem, MPFlow enables direct prediction of final graph representations using a single-step fourth-order Runge-Kutta integration scheme. 
Empirical results demonstrate that MPFlow achieves comparable accuracy to traditional message passing while significantly reducing computational complexity, making it particularly valuable for applications requiring rapid evaluation of graph properties, such as active learning in molecular design and materials discovery.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Graph Neural Networks (GNNs) have emerged as powerful tools for modeling relational data across various domains, including molecular science, social network analysis, and recommendation systems~\citep{zhou2020graph}. 
Recent years have witnessed an explosive growth in GNN applications across diverse domains. 
In molecular property prediction, GNNs have demonstrated remarkable capabilities in predicting chemical properties by modeling atomic interactions~\citep{you2018graph, lim2025cheapnet}. 
For social network analysis, GNNs excel at tasks such as link prediction by effectively capturing the topological and contextual information inherent in social graphs~\citep{zhang2018link}. 
Additionally, in recommendation systems, GNN-based approaches have shown significant improvements by modeling complex user-item relationships~\citep{fan2019graph}. 
At the core of GNNs lies the message passing mechanism, which facilitates information exchange between nodes through their connections. 
While effective, traditional GNNs often require multiple iterations of message passing to capture complex structural information, resulting in increased computational demands as network depth grows~\citep{gilmer2017neural, xu2018powerful}.

Our approach is motivated by a fundamental question: \emph{Can we predict the final node representations resulting from multiple rounds of message passing directly from the initial features?} In this paper, we introduce MPFlow, a novel flow-based surrogate model designed to predict the outcome of iterative message passing operations without explicitly computing each intermediate step.
Flow matching, a technique that models the continuous transformation path between distributions, provides an elegant mathematical framework for predicting the implicit flow of information through message passing~\citep{lipman2022flow, liu2022flow}.
By learning the continuous flow field that governs how node features evolve through multiple iterations of message passing, we can efficiently predict final node representations without explicitly computing each intermediate step. 

MPFlow represents a step toward more efficient predictions of graph neural networks by directly modeling the flow of information through graph structures, potentially enabling deeper architectures without the associated computational overhead of explicit message passing iterations~\citep{xhonneux2020continuous}. 
Our work contributes to the growing body of research aimed at enhancing the efficiency and expressiveness of GNNs. By addressing the computational bottlenecks associated with iterative message passing, MPFlow opens new possibilities for applying GNNs to larger and more complex graph-structured data, potentially enabling applications that were previously computationally infeasible.

Our key contributions are:
\begin{itemize}
\item We formulate message passing as a continuous flow matching problem, enabling direct prediction of final node representations without executing the full sequence of message passing operations.
\item We develop MPFlow, a surrogate model that leverages flow-based techniques to learn the continuous evolution of node representations during message passing, achieving high accuracy while significantly reducing computational complexity.
\item We demonstrate that MPFlow enables practical deployment of deep GNN architectures by eliminating the computational overhead of iterative message passing, making it particularly valuable for large-scale applications.
\item We show that MPFlow's efficient inference capabilities enables active learning with large pre-trained GNN models on vast search spaces, such as in materials discovery and molecular design, where rapid candidate evaluation is crucial.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

\subsection{Graph Neural Networks and Message Passing}

Graph Neural Networks (GNNs) operate on graph-structured data by iteratively updating node representations through neighborhood aggregation. 
The message passing paradigm defines this process as:

\begin{equation}
\mathbf{h}_v^{(t+1)} = \text{UPDATE}\left(\mathbf{h}_v^{(t)}, \text{AGGREGATE}\left(\{\mathbf{h}_u^{(t)} : u \in \mathcal{N}(v)\}\right)\right)
\end{equation}

where $\mathbf{h}_v^{(t)}$ represents the feature vector of node $v$ at iteration $t$, $\mathcal{N}(v)$ denotes the neighborhood of node $v$, and UPDATE and AGGREGATE are learnable functions~\citep{gilmer2017neural, hamilton2017inductive}. 
This iterative process, while effective for capturing multi-hop dependencies, incurs significant computational costs as the number of iterations increases~\citep{xu2018representation}.

Recent works have explored various approaches to mitigate this computational burden, including layer-wise parameter sharing~\citep{wu2020comprehensive}, skip connections~\citep{li2019deepgcns}, and adaptive computation techniques~\citep{huang2020combining}. 
However, these methods still require explicit computation of intermediate states during message passing.

\subsection{Continuous Normalizing Flows and Flow Matching}

Continuous normalizing flows (CNFs) model the transformation of probability distributions through ordinary differential equations (ODEs)~\citep{chen2018neural}. 
The evolution of a distribution is governed by a continuous-time flow field:

\begin{equation}
\frac{d\mathbf{x}(t)}{dt} = \mathbf{v}(\mathbf{x}(t), t)
\end{equation}

where $\mathbf{v}(\mathbf{x}, t)$ is the velocity field. Flow matching extends this concept by directly learning the vector field that transports one distribution to another~\citep{lipman2022flow}. 
This approach has shown promise in generative modeling tasks by offering stable training and efficient sampling~\citep{liu2022flow, tong2023improving}.

Our work draws inspiration from these continuous flow-based methods, adapting them to model the evolution of node representations during message passing in GNNs. 
Unlike previous applications focused primarily on generative tasks, we leverage flow matching as a surrogate model for predicting the outcome of iterative computational processes on graphs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}

\subsection{Problem Definition}

Given a graph $G = (V, E)$ with node features $\mathbf{H} \in \mathbb{R}^{|V| \times d}$, we first obtain a global graph representation through appropriate pooling operations. 
The pooling strategy depends on the type of GNN:

For invariant GNNs, we use simple mean pooling across nodes:
\begin{equation}
\mathbf{x} = \frac{1}{|V|}\sum_{v \in V}\mathbf{h}_v \in \mathbb{R}^d
\end{equation}

For equivariant GNNs with spherical harmonic representations, we pool across different frequency bands and perform weighted sum:
\begin{equation}
\mathbf{x} = \frac{1}{L_{max}+1} \sum_{l=0}^{L_\text{max}} \frac{1}{2l+1} \sum_{m=-l}^l \mathbf{H}_{l,m} \in \mathbb{R}^d
\end{equation}
where $L_\text{max}$ is the maximum spherical harmonic degree, and $\mathbf{H}_{l,m}$ represents the features in the $(l,m)$ spherical harmonic channel.

In both cases, this pooling operation transforms node-level features into fixed-dimensional global graph representations. 
For both the initial and final states of message passing:

\begin{equation}
\mathbf{x}^{(0)} = \text{Pool}(\mathbf{H}^{(0)}) \in \mathbb{R}^d
\end{equation}
\begin{equation}
\mathbf{x}^{(T)} = \text{Pool}(\mathbf{H}^{(T)}) \in \mathbb{R}^d
\end{equation}

where Pool($\cdot$) is either mean pooling for invariant GNNs or spherical harmonic pooling for equivariant GNNs. 
These global representations serve as the boundary conditions for our flow-based prediction, with $\mathbf{x}^{(0)}$ as the initial state and $\mathbf{x}^{(T)}$ as the target state.

\subsection{MPFlow: Message Passing as Flow Matching}

Let $\mathcal{V}_\theta: \mathbb{R}^d \times [0,1] \rightarrow \mathbb{R}^d$ be our parameterized velocity field that maps the current graph embedding and time to instantaneous changes in the embedding space. 
The velocity field is implemented as a neural network:

\begin{equation}
\mathcal{V}_\theta(\mathbf{x}(t), t) = f_\text{out}(f_\text{attn}(f_\text{res}(f_\text{in}(\mathbf{x}(t)) + \phi(t))))
\end{equation}

where $\mathbf{x}(t) \in \mathbb{R}^d$ represents the graph embedding at time $t$. 
The architecture comprises the following components, where $f_\text{in}: \mathbb{R}^d \rightarrow \mathbb{R}^{d_h}$ projects the input to a higher-dimensional hidden space and $f_\text{out}: \mathbb{R}^{d_h} \rightarrow \mathbb{R}^d$ projects back to the original embedding dimension. 
The intermediate components are:

\textbf{Time Embedding Network} $\phi: [0,1] \rightarrow \mathbb{R}^{d_t}$ maps scalar time values to high-dimensional representations using sinusoidal encodings:
\begin{equation}
\phi_i(t) = \begin{cases}
\sin(t \omega_i) & \text{if } i \text{ is even} \\
\cos(t \omega_i) & \text{if } i \text{ is odd}
\end{cases}, \quad \omega_i = 1/10000^{2i/d_t}
\end{equation}
where $d_t$ is the time embedding dimension.

\textbf{Residual Block} $f_\text{res}: \mathbb{R}^{d_h} \times \mathbb{R}^{d_t} \rightarrow \mathbb{R}^{d_h}$ applies time-conditioned transformations:
\begin{equation}
f_\text{res}(\mathbf{h}, \mathbf{t}) = \mathbf{h} + \text{MLP}(\text{LN}(\mathbf{h})) \odot (1 + \gamma_\theta(\mathbf{t})) + \beta_\theta(\mathbf{t})
\end{equation}
where $\text{LN}$ denotes layer normalization, and $\gamma_\theta, \beta_\theta: \mathbb{R}^{d_t} \rightarrow \mathbb{R}^{d_h}$ are learned time-dependent scaling and shifting functions.

\textbf{Multi-head Self-Attention} $f_\text{attn}: \mathbb{R}^{d_h} \rightarrow \mathbb{R}^{d_h}$ processes the hidden representation:
\begin{equation}
f_\text{attn}(\mathbf{h}) = \text{MultiHead}(\mathbf{h}, \mathbf{h}, \mathbf{h}) + \mathbf{h}
\end{equation}

The continuous evolution of the global graph representation is governed by the ODE:
\begin{equation}
\frac{d\mathbf{x}(t)}{dt} = \mathcal{V}_\theta(\mathbf{x}(t), t), \quad \mathbf{x}(0) = \mathbf{x}^{(0)}
\end{equation}

For numerical integration, we employ a single-step fourth-order Runge-Kutta (RK4) scheme. 
Given the initial global representation $\mathbf{x}^{(0)}$, we compute:

\begin{align}
\mathbf{k}_1 &= \mathcal{V}_\theta(\mathbf{x}^{(0)}, 0) \\
\mathbf{k}_2 &= \mathcal{V}_\theta(\mathbf{x}^{(0)} + 0.5\mathbf{k}_1, 0.5) \\
\mathbf{k}_3 &= \mathcal{V}_\theta(\mathbf{x}^{(0)} + 0.5\mathbf{k}_2, 0.5) \\
\mathbf{k}_4 &= \mathcal{V}_\theta(\mathbf{x}^{(0)} + \mathbf{k}_3, 1.0)
\end{align}

The final global graph representation is obtained through:

\begin{equation}
\mathbf{x}^{(T)} = \mathbf{x}^{(0)} + \frac{1}{6}(\mathbf{k}_1 + 2\mathbf{k}_2 + 2\mathbf{k}_3 + \mathbf{k}_4)
\end{equation}

This single-step RK4 integration scheme achieves fourth-order accuracy while maintaining constant computational complexity, requiring only four evaluations of the velocity field.

\subsection{Training Objective}

To train MPFlow, we leverage the principles of flow matching~\citep{lipman2022flow}. 
Given pairs of initial and final graph representations $(\mathbf{x}^{(0)}, \mathbf{x}^{(T)}) \in \mathbb{R}^d$, we optimize the velocity field to match the trajectory between these points.

The training process involves sampling random time points $t \sim \mathcal{U}(0,1)$ and computing intermediate states through linear interpolation:

\begin{equation}
\mathbf{u}_t = \mathbf{x}^{(T)} - \mathbf{x}^{(0)} \quad \text{(target direction)}
\end{equation}

\begin{equation}
\mathbf{x}_t = \mathbf{x}^{(0)} + t\mathbf{u}_t \quad \text{(intermediate state)}
\end{equation}

The training objective consists of two components:

\textbf{Flow Matching Loss} ensures the learned velocity field $\mathcal{V}_\theta$ matches the target direction:

\begin{equation}
\mathcal{L}_{\text{flow}} = \mathbb{E}_{t \sim \mathcal{U}(0,1)}\left[\|\mathcal{V}_\theta(\mathbf{x}_t, t) - \mathbf{u}_t\|_F^2\right]
\end{equation}

\textbf{Directional Alignment Loss} encourages the predicted velocity to align with the target direction:

\begin{equation}
\mathcal{L}_{\text{dir}} = \mathbb{E}_{t \sim \mathcal{U}(0,1)}\left[1 - \text{cos}\left(\mathcal{V}_\theta(\mathbf{x}_t, t), \mathbf{u}_t\right)\right]
\end{equation}

where $\text{cos}(\mathbf{a}, \mathbf{b})$ denotes the cosine similarity between vectors $\mathbf{a}$ and $\mathbf{b}$.

The combined loss function is:

\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{flow}} + \alpha \mathcal{L}_{\text{dir}}
\end{equation}

where $\alpha$ is a hyperparameter balancing the two loss terms. 

\section{Experiment}
Further details can be found at \href{https://github.com/hyukjunlim/equiformerv2}{https://github.com/hyukjunlim/equiformerv2}.

\section{Conclusion}

In this paper, we introduced MPFlow, a flow-based surrogate model that predicts the outcome of message passing in GNNs through continuous flow matching. 
By learning the velocity field that governs the evolution of global graph representations, our method bypasses the need for explicit intermediate computations while maintaining the expressivity of deep architectures. 
The empirical results demonstrate that MPFlow achieves comparable accuracy to traditional message passing while significantly reducing computational complexity.

The efficiency gains from MPFlow are particularly valuable for applications requiring rapid evaluation of graph properties, such as active learning in molecular design or materials discovery. 
Our formulation also provides a theoretical bridge between discrete message passing iterations and continuous-time graph representation dynamics, opening new perspectives on the relationship between GNN depth and representational capacity.

Future research directions include extending the framework to dynamic graphs, developing more sophisticated flow field parameterizations, and exploring connections to continuous-depth neural architectures. 
These theoretical and practical advances could further enhance our understanding of information flow in graph neural networks while enabling more efficient learning on large-scale graph-structured data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{ack}
% Placeholders.
% \end{ack}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{neurips_2024}

% \section*{References}
% {
% \small

% % [1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
% % connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
% % (eds.), {\it Advances in Neural Information Processing Systems 7},
% % pp.\ 609--616. Cambridge, MA: MIT Press.

% }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \appendix

% \section{Appendix / supplemental material}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph{Paragraphs}
% Placeholders.

% \begin{figure}
%   \centering
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
% \end{figure}


% \begin{table}
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule(r){1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}



\end{document}